{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a2e7d62b-5779-4211-822c-457c77321f8b",
   "metadata": {},
   "source": [
    "# Convert and Optimize YOLOv8 real-time object detection with OpenVINO™\n",
    "\n",
    "Real-time object detection is often used as a key component in computer vision systems.\n",
    "Applications that use real-time object detection models include video analytics, robotics, autonomous vehicles, multi-object tracking and object counting, medical image analysis, and many others.\n",
    "\n",
    "\n",
    "This tutorial demonstrates step-by-step instructions on how to run and optimize PyTorch YOLOv8 with OpenVINO. We consider the steps required for object detection scenario.\n",
    "\n",
    "The tutorial consists of the following steps:\n",
    "- Prepare the PyTorch model.\n",
    "- Download and prepare a dataset.\n",
    "- Validate the original model.\n",
    "- Convert the PyTorch model to OpenVINO IR.\n",
    "- Validate the converted model.\n",
    "- Prepare and run optimization pipeline.\n",
    "- Compare performance of the FP32 and quantized models.\n",
    "- Compare accuracy of the FP32 and quantized models.\n",
    "- Other optimization possibilities with OpenVINO api\n",
    "- Live demo\n",
    "\n",
    "\n",
    "\n",
    "#### Table of contents:\n",
    "\n",
    "- [Get PyTorch model](#Get-PyTorch-model)\n",
    "    - [Prerequisites](#Prerequisites)\n",
    "- [Instantiate model](#Instantiate-model)\n",
    "    - [Convert model to OpenVINO IR](#Convert-model-to-OpenVINO-IR)\n",
    "    - [Verify model inference](#Verify-model-inference)\n",
    "    - [Select inference device](#Select-inference-device)\n",
    "    - [Test on single image](#Test-on-single-image)\n",
    "- [Check model accuracy on the dataset](#Check-model-accuracy-on-the-dataset)\n",
    "    - [Download the validation dataset](#Download-the-validation-dataset)\n",
    "    - [Define validation function](#Define-validation-function)\n",
    "    - [Configure Validator helper and create DataLoader](#Configure-Validator-helper-and-create-DataLoader)\n",
    "- [Optimize model using NNCF Post-training Quantization API](#Optimize-model-using-NNCF-Post-training-Quantization-API)\n",
    "    - [Validate Quantized model inference](#Validate-Quantized-model-inference)\n",
    "- [Compare the Original and Quantized Models](#Compare-the-Original-and-Quantized-Models)\n",
    "    - [Compare performance object detection models](#Compare-performance-object-detection-models)\n",
    "    - [Validate quantized model accuracy](#Validate-quantized-model-accuracy)\n",
    "- [Next steps](#Next-steps)\n",
    "    - [Async inference pipeline](#Async-inference-pipeline)\n",
    "    - [Integration preprocessing to model](#Integration-preprocessing-to-model)\n",
    "        - [Initialize PrePostProcessing API](#Initialize-PrePostProcessing-API)\n",
    "        - [Define input data format](#Define-input-data-format)\n",
    "        - [Describe preprocessing steps](#Describe-preprocessing-steps)\n",
    "        - [Integrating Steps into a Model](#Integrating-Steps-into-a-Model)\n",
    "        - [Postprocessing](#Postprocessing)\n",
    "- [Live demo](#Live-demo)\n",
    "    - [Run Live Object Detection](#Run-Live-Object-Detection)\n",
    "\n",
    "\n",
    "### Installation Instructions\n",
    "\n",
    "This is a self-contained example that relies solely on its own code.\n",
    "\n",
    "We recommend  running the notebook in a virtual environment. You only need a Jupyter server to start.\n",
    "For details, please refer to [Installation Guide](https://github.com/openvinotoolkit/openvino_notebooks/blob/latest/README.md#-installation-guide).\n",
    "\n",
    "<img referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=5b5a4db0-7875-4bfb-bdbd-01698b5b1a77&file=notebooks/yolov8-optimization/yolov8-object-detection.ipynb\" />\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d7a12678-b12f-48d1-9735-398855733e46",
   "metadata": {},
   "source": [
    "## Get PyTorch model\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Generally, PyTorch models represent an instance of the [`torch.nn.Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) class, initialized by a state dictionary with model weights.\n",
    "We will use the YOLOv8 nano model (also known as `yolov8n`) pre-trained on a COCO dataset, which is available in this [repo](https://github.com/ultralytics/ultralytics). Similar steps are also applicable to other YOLOv8 models.\n",
    "Typical steps to obtain a pre-trained model:\n",
    "1. Create an instance of a model class.\n",
    "2. Load a checkpoint state dict, which contains the pre-trained model weights.\n",
    "3. Turn the model to evaluation for switching some operations to inference mode.\n",
    "\n",
    "In this case, the creators of the model provide an API that enables converting the YOLOv8 model to ONNX and then to OpenVINO IR. Therefore, we do not need to do these steps manually."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e2267760-cbfe-41c6-958d-cad9f845d5bb",
   "metadata": {},
   "source": [
    "#### Prerequisites\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Install necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d04872-6916-454c-9211-6c644b50dc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q \"openvino>=2024.0.0\" \"nncf>=2.9.0\"\n",
    "%pip install -q \"torch>=2.1\" \"torchvision>=0.16\" \"ultralytics==8.2.24\" onnx tqdm opencv-python --extra-index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1bbe319c",
   "metadata": {},
   "source": [
    "Import required utility functions.\n",
    "The lower cell will download the `notebook_utils` Python module from GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f6cd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Fetch `notebook_utils` module\n",
    "import requests\n",
    "\n",
    "r = requests.get(\n",
    "    url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/notebook_utils.py\",\n",
    ")\n",
    "\n",
    "open(\"notebook_utils.py\", \"w\").write(r.text)\n",
    "\n",
    "from notebook_utils import download_file, VideoPlayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373658bd-7e64-4479-914e-f2742d330afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download a test sample\n",
    "IMAGE_PATH = Path(\"./data/coco_bike.jpg\")\n",
    "download_file(\n",
    "    url=\"https://storage.openvinotoolkit.org/repositories/openvino_notebooks/data/data/image/coco_bike.jpg\",\n",
    "    filename=IMAGE_PATH.name,\n",
    "    directory=IMAGE_PATH.parent,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ee32fd08-650c-4751-bb41-d8afccb2495e",
   "metadata": {},
   "source": [
    "## Instantiate model\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "There are [several models](https://docs.ultralytics.com/tasks/detect/) available in the original repository, targeted for different tasks. For loading the model, required to specify a path to the model checkpoint. It can be some local path or name available on models hub (in this case model checkpoint will be downloaded automatically). \n",
    "\n",
    "Making prediction, the model accepts a path to input image and returns list with Results class object. Results contains boxes for object detection model. Also it contains utilities for processing results, for example, `plot()` method for drawing.\n",
    "\n",
    "Let us consider the examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae2543b",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = Path(\"./models\")\n",
    "models_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fdb05e-02a6-48f6-ac64-7199f0c331fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from ultralytics import YOLO\n",
    "\n",
    "DET_MODEL_NAME = \"yolov8n\"\n",
    "\n",
    "det_model = YOLO(models_dir / f\"{DET_MODEL_NAME}.pt\")\n",
    "label_map = det_model.model.names\n",
    "\n",
    "res = det_model(IMAGE_PATH)\n",
    "Image.fromarray(res[0].plot()[:, :, ::-1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e345ffcc-c4b8-44ba-8b03-f37e63a060da",
   "metadata": {},
   "source": [
    "### Convert model to OpenVINO IR\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "YOLOv8 provides API for convenient model exporting to different formats including OpenVINO IR. `model.export` is responsible for model conversion. We need to specify the format, and additionally, we can preserve dynamic shapes in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2cc576-50c9-409f-be86-ad7122dce24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# object detection model\n",
    "det_model_path = models_dir / f\"{DET_MODEL_NAME}_openvino_model/{DET_MODEL_NAME}.xml\"\n",
    "if not det_model_path.exists():\n",
    "    det_model.export(format=\"openvino\", dynamic=True, half=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "713cd45f-2b19-4a1e-bc9d-5e69bd95d896",
   "metadata": {},
   "source": [
    "### Verify model inference\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "We can reuse the base model pipeline for pre- and postprocessing just replacing the inference method where we will use the IR model for inference."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "462338cc",
   "metadata": {},
   "source": [
    "### Select inference device\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Select device from dropdown list for running inference using OpenVINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e49089e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "import openvino as ov\n",
    "\n",
    "core = ov.Core()\n",
    "\n",
    "device = widgets.Dropdown(\n",
    "    options=core.available_devices + [\"AUTO\"],\n",
    "    value=\"AUTO\",\n",
    "    description=\"Device:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cd163eab-e803-4ae8-96da-22c3bf303630",
   "metadata": {},
   "source": [
    "### Test on single image\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Now, once we have defined preprocessing and postprocessing steps, we are ready to check model prediction for object detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97b41c0-02d4-4357-90a1-9b727f0fcf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "core = ov.Core()\n",
    "\n",
    "det_ov_model = core.read_model(det_model_path)\n",
    "\n",
    "ov_config = {}\n",
    "if device.value != \"CPU\":\n",
    "    det_ov_model.reshape({0: [1, 3, 640, 640]})\n",
    "if \"GPU\" in device.value or (\"AUTO\" in device.value and \"GPU\" in core.available_devices):\n",
    "    ov_config = {\"GPU_DISABLE_WINOGRAD_CONVOLUTION\": \"YES\"}\n",
    "det_compiled_model = core.compile_model(det_ov_model, device.value, ov_config)\n",
    "\n",
    "\n",
    "def infer(*args):\n",
    "    result = det_compiled_model(args)\n",
    "    return torch.from_numpy(result[0])\n",
    "\n",
    "\n",
    "det_model.predictor.inference = infer\n",
    "det_model.predictor.model.pt = False\n",
    "\n",
    "res = det_model(IMAGE_PATH)\n",
    "Image.fromarray(res[0].plot()[:, :, ::-1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ae5b0855-4bcd-46cc-a0a8-f13a693922e9",
   "metadata": {},
   "source": [
    "## Check model accuracy on the dataset\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "For comparing the optimized model result with the original, it is good to know some measurable results in terms of model accuracy on the validation dataset. \n",
    "\n",
    "\n",
    "### Download the validation dataset\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "YOLOv8 is pre-trained on the COCO dataset, so to evaluate the model accuracy we need to download it. According to the instructions provided in the YOLOv8 repo, we also need to download annotations in the format used by the author of the model, for use with the original model evaluation function.\n",
    "\n",
    ">**Note**: The initial dataset download may take a few minutes to complete. The download speed will vary depending on the quality of your internet connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a3a3a2-5141-4cfd-bbf1-84dbb5e0bc08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "\n",
    "from ultralytics.data.utils import DATASETS_DIR\n",
    "\n",
    "\n",
    "DATA_URL = \"http://images.cocodataset.org/zips/val2017.zip\"\n",
    "LABELS_URL = \"https://github.com/ultralytics/yolov5/releases/download/v1.0/coco2017labels-segments.zip\"\n",
    "CFG_URL = \"https://raw.githubusercontent.com/ultralytics/ultralytics/v8.1.0/ultralytics/cfg/datasets/coco.yaml\"\n",
    "\n",
    "OUT_DIR = DATASETS_DIR\n",
    "\n",
    "DATA_PATH = OUT_DIR / \"val2017.zip\"\n",
    "LABELS_PATH = OUT_DIR / \"coco2017labels-segments.zip\"\n",
    "CFG_PATH = OUT_DIR / \"coco.yaml\"\n",
    "\n",
    "download_file(DATA_URL, DATA_PATH.name, DATA_PATH.parent)\n",
    "download_file(LABELS_URL, LABELS_PATH.name, LABELS_PATH.parent)\n",
    "download_file(CFG_URL, CFG_PATH.name, CFG_PATH.parent)\n",
    "\n",
    "if not (OUT_DIR / \"coco/labels\").exists():\n",
    "    with ZipFile(LABELS_PATH, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(OUT_DIR)\n",
    "    with ZipFile(DATA_PATH, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(OUT_DIR / \"coco/images\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0f516ba2-e54c-416c-893a-cbcd2228e140",
   "metadata": {},
   "source": [
    "### Define validation function\n",
    "[back to top ⬆️](#Table-of-contents:)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96bd01e-5b92-49be-b7b8-9269ace1398b",
   "metadata": {
    "test_replace": {
     "int = None": "int = 100"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from ultralytics.utils.metrics import ConfusionMatrix\n",
    "\n",
    "\n",
    "def test(\n",
    "    model: ov.Model,\n",
    "    core: ov.Core,\n",
    "    data_loader: torch.utils.data.DataLoader,\n",
    "    validator,\n",
    "    num_samples: int = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    OpenVINO YOLOv8 model accuracy validation function. Runs model validation on dataset and returns metrics\n",
    "    Parameters:\n",
    "        model (Model): OpenVINO model\n",
    "        data_loader (torch.utils.data.DataLoader): dataset loader\n",
    "        validator: instance of validator class\n",
    "        num_samples (int, *optional*, None): validate model only on specified number samples, if provided\n",
    "    Returns:\n",
    "        stats: (Dict[str, float]) - dictionary with aggregated accuracy metrics statistics, key is metric name, value is metric value\n",
    "    \"\"\"\n",
    "    validator.seen = 0\n",
    "    validator.jdict = []\n",
    "    validator.stats = dict(tp=[], conf=[], pred_cls=[], target_cls=[])\n",
    "    validator.batch_i = 1\n",
    "    validator.confusion_matrix = ConfusionMatrix(nc=validator.nc)\n",
    "    model.reshape({0: [1, 3, -1, -1]})\n",
    "    compiled_model = core.compile_model(model)\n",
    "    for batch_i, batch in enumerate(tqdm(data_loader, total=num_samples)):\n",
    "        if num_samples is not None and batch_i == num_samples:\n",
    "            break\n",
    "        batch = validator.preprocess(batch)\n",
    "        results = compiled_model(batch[\"img\"])\n",
    "        preds = torch.from_numpy(results[compiled_model.output(0)])\n",
    "        preds = validator.postprocess(preds)\n",
    "        validator.update_metrics(preds, batch)\n",
    "    stats = validator.get_stats()\n",
    "    return stats\n",
    "\n",
    "\n",
    "def print_stats(stats: np.ndarray, total_images: int, total_objects: int):\n",
    "    \"\"\"\n",
    "    Helper function for printing accuracy statistic\n",
    "    Parameters:\n",
    "        stats: (Dict[str, float]) - dictionary with aggregated accuracy metrics statistics, key is metric name, value is metric value\n",
    "        total_images (int) -  number of evaluated images\n",
    "        total objects (int)\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    print(\"Boxes:\")\n",
    "    mp, mr, map50, mean_ap = (\n",
    "        stats[\"metrics/precision(B)\"],\n",
    "        stats[\"metrics/recall(B)\"],\n",
    "        stats[\"metrics/mAP50(B)\"],\n",
    "        stats[\"metrics/mAP50-95(B)\"],\n",
    "    )\n",
    "    # Print results\n",
    "    print(\"    Best mean average:\")\n",
    "    s = (\"%20s\" + \"%12s\" * 6) % (\n",
    "        \"Class\",\n",
    "        \"Images\",\n",
    "        \"Labels\",\n",
    "        \"Precision\",\n",
    "        \"Recall\",\n",
    "        \"mAP@.5\",\n",
    "        \"mAP@.5:.95\",\n",
    "    )\n",
    "    print(s)\n",
    "    pf = \"%20s\" + \"%12i\" * 2 + \"%12.3g\" * 4  # print format\n",
    "    print(pf % (\"all\", total_images, total_objects, mp, mr, map50, mean_ap))\n",
    "    if \"metrics/precision(M)\" in stats:\n",
    "        s_mp, s_mr, s_map50, s_mean_ap = (\n",
    "            stats[\"metrics/precision(M)\"],\n",
    "            stats[\"metrics/recall(M)\"],\n",
    "            stats[\"metrics/mAP50(M)\"],\n",
    "            stats[\"metrics/mAP50-95(M)\"],\n",
    "        )\n",
    "        # Print results\n",
    "        print(\"    Macro average mean:\")\n",
    "        s = (\"%20s\" + \"%12s\" * 6) % (\n",
    "            \"Class\",\n",
    "            \"Images\",\n",
    "            \"Labels\",\n",
    "            \"Precision\",\n",
    "            \"Recall\",\n",
    "            \"mAP@.5\",\n",
    "            \"mAP@.5:.95\",\n",
    "        )\n",
    "        print(s)\n",
    "        pf = \"%20s\" + \"%12i\" * 2 + \"%12.3g\" * 4  # print format\n",
    "        print(pf % (\"all\", total_images, total_objects, s_mp, s_mr, s_map50, s_mean_ap))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "41ee1a66-c2ca-413b-883c-497f718337e6",
   "metadata": {},
   "source": [
    "### Configure Validator helper and create DataLoader\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "The original model repository uses a `Validator` wrapper, which represents the accuracy validation pipeline. It creates dataloader and evaluation metrics and updates metrics on each data batch produced by the dataloader. Besides that, it is responsible for data preprocessing and results postprocessing. For class initialization, the configuration should be provided. We will use the default setup, but it can be replaced with some parameters overriding to test on custom data. The model has connected the `ValidatorClass` method, which creates a validator class instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878473e3-7638-4471-bd8e-31350b119f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics.utils import DEFAULT_CFG\n",
    "from ultralytics.cfg import get_cfg\n",
    "from ultralytics.data.converter import coco80_to_coco91_class\n",
    "from ultralytics.data.utils import check_det_dataset\n",
    "\n",
    "args = get_cfg(cfg=DEFAULT_CFG)\n",
    "args.data = str(CFG_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466729b5-4fad-4de2-a1d4-2952718cbe7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "det_validator = det_model.task_map[det_model.task][\"validator\"](args=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccd7ba9-6066-4328-9563-4f1a740bca9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "det_validator.data = check_det_dataset(args.data)\n",
    "det_validator.stride = 32\n",
    "det_data_loader = det_validator.get_dataloader(OUT_DIR / \"coco\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808a8b42-4ca0-429f-b40b-4384c83aad61",
   "metadata": {},
   "outputs": [],
   "source": [
    "det_validator.is_coco = True\n",
    "det_validator.class_map = coco80_to_coco91_class()\n",
    "det_validator.names = det_model.model.names\n",
    "det_validator.metrics.names = det_validator.names\n",
    "det_validator.nc = det_model.model.model[-1].nc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "87a73bbc-d0e0-494e-9533-26d51295a196",
   "metadata": {},
   "source": [
    "After definition test function and validator creation, we are ready for getting accuracy metrics\n",
    ">**Note**: Model evaluation is time consuming process and can take several minutes, depending on the hardware. For reducing calculation time, we define `num_samples` parameter with evaluation subset size, but in this case, accuracy can be noncomparable with originally reported by the authors of the model, due to validation subset difference.\n",
    "*To validate the models on the full dataset set `NUM_TEST_SAMPLES = None`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e7bb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TEST_SAMPLES = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d2df4b-4e03-4c5d-a329-9fae35fdf7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_det_stats = test(det_ov_model, core, det_data_loader, det_validator, num_samples=NUM_TEST_SAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b396bb40-9c0c-48c3-a485-c4642dc6bf1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_stats(fp_det_stats, det_validator.seen, det_validator.nt_per_class.sum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8d7f89f3-4a8f-4b1c-a582-964e111e51dc",
   "metadata": {},
   "source": [
    "`print_stats` reports the following list of accuracy metrics:\n",
    "\n",
    "* `Precision` is the degree of exactness of the model in identifying only relevant objects. \n",
    "* `Recall` measures the ability of the model to detect all ground truths objects.\n",
    "* `mAP@t` - mean average precision, represented as area under the Precision-Recall curve aggregated over all classes in the dataset, where `t` is the Intersection Over Union (IOU) threshold, degree of overlapping between ground truth and predicted objects. Therefore, `mAP@.5` indicates that mean average precision is calculated at 0.5 IOU threshold, `mAP@.5:.95` - is calculated on range IOU thresholds from 0.5 to 0.95 with step 0.05."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "13b69e12-2e22-44c1-bfed-fdc88f6c424d",
   "metadata": {},
   "source": [
    "## Optimize model using NNCF Post-training Quantization API\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "[NNCF](https://github.com/openvinotoolkit/nncf) provides a suite of advanced algorithms for Neural Networks inference optimization in OpenVINO with minimal accuracy drop.\n",
    "We will use 8-bit quantization in post-training mode (without the fine-tuning pipeline) to optimize YOLOv8.\n",
    "\n",
    "The optimization process contains the following steps:\n",
    "\n",
    "1. Create a Dataset for quantization.\n",
    "2. Run `nncf.quantize` for getting an optimized model.\n",
    "3. Serialize OpenVINO IR model, using the `openvino.runtime.serialize` function."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "72ba8180-5239-43bc-acdd-acbc150aeb78",
   "metadata": {},
   "source": [
    "Please select below whether you would like to run quantization to improve model inference speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f13acd-5573-49a6-b68a-5fa177f492ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "int8_model_det_path = models_dir / f\"{DET_MODEL_NAME}_openvino_int8_model/{DET_MODEL_NAME}.xml\"\n",
    "\n",
    "to_quantize = widgets.Checkbox(\n",
    "    value=True,\n",
    "    description=\"Quantization\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "to_quantize"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "42c4069a-1832-4652-8bae-f493962b5e87",
   "metadata": {},
   "source": [
    "Let's load `skip magic` extension to skip quantization if `to_quantize` is not selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdddb2bc-7f4a-4219-abf0-8bbb56acd95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch skip_kernel_extension module\n",
    "r = requests.get(\n",
    "    url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/skip_kernel_extension.py\",\n",
    ")\n",
    "open(\"skip_kernel_extension.py\", \"w\").write(r.text)\n",
    "\n",
    "%load_ext skip_kernel_extension"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7d7eeae8-f3f4-4e95-b1a6-a23085f03ebc",
   "metadata": {},
   "source": [
    "Reuse validation dataloader in accuracy testing for quantization. \n",
    "For that, it should be wrapped into the `nncf.Dataset` object and define a transformation function for getting only input tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd994958-6988-4a1d-ac7f-3efbd97135cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip not $to_quantize.value\n",
    "\n",
    "import nncf\n",
    "from typing import Dict\n",
    "\n",
    "\n",
    "def transform_fn(data_item:Dict):\n",
    "    \"\"\"\n",
    "    Quantization transform function. Extracts and preprocess input data from dataloader item for quantization.\n",
    "    Parameters:\n",
    "       data_item: Dict with data item produced by DataLoader during iteration\n",
    "    Returns:\n",
    "        input_tensor: Input data for quantization\n",
    "    \"\"\"\n",
    "    input_tensor = det_validator.preprocess(data_item)['img'].numpy()\n",
    "    return input_tensor\n",
    "\n",
    "\n",
    "quantization_dataset = nncf.Dataset(det_data_loader, transform_fn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "91629284-e261-494d-9464-146ed7190084",
   "metadata": {},
   "source": [
    "The `nncf.quantize` function provides an interface for model quantization. It requires an instance of the OpenVINO Model and quantization dataset. \n",
    "Optionally, some additional parameters for the configuration quantization process (number of samples for quantization, preset, ignored scope, etc.) can be provided. YOLOv8 model contains non-ReLU activation functions, which require asymmetric quantization of activations. To achieve a better result, we will use a `mixed` quantization preset. It provides symmetric quantization of weights and asymmetric quantization of activations. For more accurate results, we should keep the operation in the postprocessing subgraph in floating point precision, using the `ignored_scope` parameter.\n",
    "\n",
    ">**Note**: Model post-training quantization is time-consuming process. Be patient, it can take several minutes depending on your hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e8cec4-d0b3-4da0-b54c-7964e7bcbfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip not $to_quantize.value\n",
    "\n",
    "ignored_scope = nncf.IgnoredScope( # post-processing\n",
    "    subgraphs=[\n",
    "        nncf.Subgraph(inputs=['__module.model.22/aten::cat/Concat',\n",
    "                              '__module.model.22/aten::cat/Concat_1',\n",
    "                              '__module.model.22/aten::cat/Concat_2'],\n",
    "                      outputs=['__module.model.22/aten::cat/Concat_7'])\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Detection model\n",
    "quantized_det_model = nncf.quantize(\n",
    "    det_ov_model,\n",
    "    quantization_dataset,\n",
    "    preset=nncf.QuantizationPreset.MIXED,\n",
    "    ignored_scope=ignored_scope\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab02faf1-196c-4f91-93d1-c86b4cae93d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip not $to_quantize.value\n",
    "\n",
    "print(f\"Quantized detection model will be saved to {int8_model_det_path}\")\n",
    "ov.save_model(quantized_det_model, str(int8_model_det_path))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c8c92a68-3eb8-4ea5-9e35-d496f45b3cc3",
   "metadata": {},
   "source": [
    "### Validate Quantized model inference\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "`nncf.quantize` returns the OpenVINO Model class instance, which is suitable for loading on a device for making predictions. `INT8` model input data and output result formats have no difference from the floating point model representation. Therefore, we can reuse the same `detect` function defined above for getting the `INT8` model result on the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b24588",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip not $to_quantize.value\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2273f037-d091-4d63-af1e-eed6b41fa6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip not $to_quantize.value\n",
    "\n",
    "ov_config = {}\n",
    "if device.value != \"CPU\":\n",
    "    quantized_det_model.reshape({0: [1, 3, 640, 640]})\n",
    "if \"GPU\" in device.value or (\"AUTO\" in device.value and \"GPU\" in core.available_devices):\n",
    "    ov_config = {\"GPU_DISABLE_WINOGRAD_CONVOLUTION\": \"YES\"}\n",
    "quantized_det_compiled_model = core.compile_model(quantized_det_model, device.value, ov_config)\n",
    "\n",
    "\n",
    "def infer(*args):\n",
    "    result = quantized_det_compiled_model(args)\n",
    "    return torch.from_numpy(result[0])\n",
    "\n",
    "det_model.predictor.inference = infer\n",
    "\n",
    "res = det_model(IMAGE_PATH)\n",
    "display(Image.fromarray(res[0].plot()[:, :, ::-1]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "59081cae-9c92-46f0-8117-ed8c6fa6ff19",
   "metadata": {},
   "source": [
    "## Compare the Original and Quantized Models\n",
    "[back to top ⬆️](#Table-of-contents:)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0cd927bc-d8f2-4cd0-95f1-88d977c4b4e3",
   "metadata": {},
   "source": [
    "### Compare performance object detection models\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Finally, use the OpenVINO [Benchmark Tool](https://docs.openvino.ai/2024/learn-openvino/openvino-samples/benchmark-tool.html) to measure the inference performance of the `FP32` and `INT8` models.\n",
    "\n",
    "> **Note**: For more accurate performance, it is recommended to run `benchmark_app` in a terminal/command prompt after closing other applications. Run `benchmark_app -m <model_path> -d CPU -shape \"<input_shape>\"` to benchmark async inference on CPU on specific input data shape for one minute. Change `CPU` to `GPU` to benchmark on GPU. Run `benchmark_app --help` to see an overview of all command-line options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d249cbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip not $to_quantize.value\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b07129-ec21-40e0-9885-1928ace8a55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if int8_model_det_path.exists():\n",
    "    # Inference FP32 model (OpenVINO IR)\n",
    "    !benchmark_app -m $det_model_path -d $device.value -api async -shape \"[1,3,640,640]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646cafce-8042-4b94-aead-de804b2051a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if int8_model_det_path.exists():\n",
    "    # Inference INT8 model (OpenVINO IR)\n",
    "    !benchmark_app -m $int8_model_det_path -d $device.value -api async -shape \"[1,3,640,640]\" -t 15"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f800b975-d001-4947-ac20-6be9fd737a95",
   "metadata": {},
   "source": [
    "### Validate quantized model accuracy\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "As we can see, there is no significant difference between `INT8` and float model result in a single image test. To understand how quantization influences model prediction precision, we can compare model accuracy on a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48299ffc-99b0-4b27-b365-6b0b8beabd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip not $to_quantize.value\n",
    "\n",
    "int8_det_stats = test(quantized_det_model, core, det_data_loader, det_validator, num_samples=NUM_TEST_SAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150e5f85-cf0f-4362-9c9e-7be8957ccc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip not $to_quantize.value\n",
    "\n",
    "print(\"FP32 model accuracy\")\n",
    "print_stats(fp_det_stats, det_validator.seen, det_validator.nt_per_class.sum())\n",
    "\n",
    "print(\"INT8 model accuracy\")\n",
    "print_stats(int8_det_stats, det_validator.seen, det_validator.nt_per_class.sum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d4a8dd76-3de3-4d4c-baef-bfd90400b127",
   "metadata": {},
   "source": [
    "Great! Looks like accuracy was changed, but not significantly and it meets passing criteria."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "184bcebd-588d-4b4d-a60a-0fa753a07ef9",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "This section contains suggestions on how to additionally improve the performance of your application using OpenVINO."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "710f5266-a114-4180-8da2-f13b33995a99",
   "metadata": {},
   "source": [
    "### Async inference pipeline\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "The key advantage of the Async API is that when a device is busy with inference, the application can perform other tasks in parallel (for example, populating inputs or scheduling other requests) rather than wait for the current inference to complete first. To understand how to perform async inference using openvino, refer to [Async API tutorial](../async-api/async-api.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7826c42b-778e-4505-86b8-ebf1bbe14d19",
   "metadata": {},
   "source": [
    "### Integration preprocessing to model\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Preprocessing API enables making preprocessing a part of the model reducing application code and dependency on additional image processing libraries. \n",
    "The main advantage of Preprocessing API is that preprocessing steps will be integrated into the execution graph and will be performed on a selected device (CPU/GPU etc.) rather than always being executed on CPU as part of an application. This will improve selected device utilization.\n",
    "\n",
    "For more information, refer to the overview of [Preprocessing API](https://docs.openvino.ai/2024/openvino-workflow/running-inference/optimize-inference/optimize-preprocessing/preprocessing-api-details.html).\n",
    "\n",
    "For example, we can integrate converting input data layout and normalization defined in `image_to_tensor` function.\n",
    "\n",
    "The integration process consists of the following steps:\n",
    "1. Initialize a PrePostProcessing object.\n",
    "2. Define the input data format.\n",
    "3. Describe preprocessing steps.\n",
    "4. Integrating Steps into a Model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "42a07a18-74ef-4802-89d5-2ae99acbf515",
   "metadata": {},
   "source": [
    "#### Initialize PrePostProcessing API\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "The `openvino.preprocess.PrePostProcessor` class enables specifying preprocessing and postprocessing steps for a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6582080b-d5fb-4662-a8b8-c4a14c9998eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openvino.preprocess import PrePostProcessor\n",
    "\n",
    "ppp = PrePostProcessor(quantized_det_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f249e086-eb5b-4caa-aeca-9fa6d5326c37",
   "metadata": {},
   "source": [
    "#### Define input data format\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "To address particular input of a model/preprocessor, the `input(input_id)` method, where `input_id` is a positional index or input tensor name for input in `model.inputs`, if a model has a single input, `input_id` can be omitted.\n",
    "After reading the image from the disc, it contains U8 pixels in the `[0, 255]` range and is stored in the `NHWC` layout. To perform a preprocessing conversion, we should provide this to the tensor description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e200bcb0-aea1-4229-ae5c-fe0fc45989a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppp.input(0).tensor().set_shape([1, 640, 640, 3]).set_element_type(ov.Type.u8).set_layout(ov.Layout(\"NHWC\"))\n",
    "pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9c5a0233-45c9-496b-945b-4e563c5066ff",
   "metadata": {},
   "source": [
    "To perform layout conversion, we also should provide information about layout expected by model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3fb1e999-6798-4de9-9ec5-5ded4f77d1ac",
   "metadata": {},
   "source": [
    "#### Describe preprocessing steps\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Our preprocessing function contains the following steps:\n",
    "* Convert the data type from `U8` to `FP32`.\n",
    "* Convert the data layout from `NHWC` to `NCHW` format.\n",
    "* Normalize each pixel by dividing on scale factor 255.\n",
    "\n",
    "`ppp.input(input_id).preprocess()` is used for defining a sequence of preprocessing steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191b4885-ecba-43e0-b88d-dc2fbaa342f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppp.input(0).preprocess().convert_element_type(ov.Type.f32).convert_layout(ov.Layout(\"NCHW\")).scale([255.0, 255.0, 255.0])\n",
    "\n",
    "print(ppp)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8a5ab2ca-3082-4d63-914d-4c1c4e5b9a6b",
   "metadata": {},
   "source": [
    "#### Integrating Steps into a Model\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Once the preprocessing steps have been finished, the model can be finally built. Additionally, we can save a completed model to OpenVINO IR, using `openvino.runtime.serialize`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63352458-b798-4880-8e57-e291483499f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model_with_preprocess = ppp.build()\n",
    "ov.save_model(\n",
    "    quantized_model_with_preprocess,\n",
    "    str(int8_model_det_path.with_name(f\"{DET_MODEL_NAME}_with_preprocess.xml\")),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "636f449e-36a9-42b3-af5b-4995036ba391",
   "metadata": {},
   "source": [
    "The model with integrated preprocessing is ready for loading to a device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8a6c2f-d812-4c7c-8cce-3030b2595d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Dict\n",
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics.utils.plotting import colors\n",
    "\n",
    "\n",
    "def plot_one_box(\n",
    "    box: np.ndarray,\n",
    "    img: np.ndarray,\n",
    "    color: Tuple[int, int, int] = None,\n",
    "    label: str = None,\n",
    "    line_thickness: int = 5,\n",
    "):\n",
    "    \"\"\"\n",
    "    Helper function for drawing single bounding box on image\n",
    "    Parameters:\n",
    "        x (np.ndarray): bounding box coordinates in format [x1, y1, x2, y2]\n",
    "        img (no.ndarray): input image\n",
    "        color (Tuple[int, int, int], *optional*, None): color in BGR format for drawing box, if not specified will be selected randomly\n",
    "        label (str, *optonal*, None): box label string, if not provided will not be provided as drowing result\n",
    "        line_thickness (int, *optional*, 5): thickness for box drawing lines\n",
    "    \"\"\"\n",
    "    # Plots one bounding box on image img\n",
    "    tl = line_thickness or round(0.002 * (img.shape[0] + img.shape[1]) / 2) + 1  # line/font thickness\n",
    "    color = color or [random.randint(0, 255) for _ in range(3)]\n",
    "    c1, c2 = (int(box[0]), int(box[1])), (int(box[2]), int(box[3]))\n",
    "    cv2.rectangle(img, c1, c2, color, thickness=tl, lineType=cv2.LINE_AA)\n",
    "    if label:\n",
    "        tf = max(tl - 1, 1)  # font thickness\n",
    "        t_size = cv2.getTextSize(label, 0, fontScale=tl / 3, thickness=tf)[0]\n",
    "        c2 = c1[0] + t_size[0], c1[1] - t_size[1] - 3\n",
    "        cv2.rectangle(img, c1, c2, color, -1, cv2.LINE_AA)  # filled\n",
    "        cv2.putText(\n",
    "            img,\n",
    "            label,\n",
    "            (c1[0], c1[1] - 2),\n",
    "            0,\n",
    "            tl / 3,\n",
    "            [225, 255, 255],\n",
    "            thickness=tf,\n",
    "            lineType=cv2.LINE_AA,\n",
    "        )\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "def draw_results(results: Dict, source_image: np.ndarray, label_map: Dict):\n",
    "    \"\"\"\n",
    "    Helper function for drawing bounding boxes on image\n",
    "    Parameters:\n",
    "        image_res (np.ndarray): detection predictions in format [x1, y1, x2, y2, score, label_id]\n",
    "        source_image (np.ndarray): input image for drawing\n",
    "        label_map; (Dict[int, str]): label_id to class name mapping\n",
    "    Returns:\n",
    "        Image with boxes\n",
    "    \"\"\"\n",
    "    boxes = results[\"det\"]\n",
    "    for idx, (*xyxy, conf, lbl) in enumerate(boxes):\n",
    "        label = f\"{label_map[int(lbl)]} {conf:.2f}\"\n",
    "        source_image = plot_one_box(xyxy, source_image, label=label, color=colors(int(lbl)), line_thickness=1)\n",
    "    return source_image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6a3ce0fc-1e80-48ba-b2e5-177607fbde57",
   "metadata": {},
   "source": [
    "##### Postprocessing\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "The model output contains detection boxes candidates, it is a tensor with the [-1,84,-1] shape in the B,84,N format, where:\n",
    "\n",
    "B - batch size\n",
    "N - number of detection boxes\n",
    "For getting the final prediction, we need to apply a non-maximum suppression algorithm and rescale box coordinates to the original image size.\n",
    "\n",
    "Finally, detection box has the [x, y, h, w, class_no_1, ..., class_no_80] format, where:\n",
    "\n",
    "(x, y) - raw coordinates of box center\n",
    "h, w - raw height and width of the box\n",
    "class_no_1, ..., class_no_80 - probability distribution over the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcbdc34-6625-48dd-9483-4a091eb1477f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "from ultralytics.utils import ops\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def letterbox(\n",
    "    img: np.ndarray,\n",
    "    new_shape: Tuple[int, int] = (640, 640),\n",
    "    color: Tuple[int, int, int] = (114, 114, 114),\n",
    "    auto: bool = False,\n",
    "    scale_fill: bool = False,\n",
    "    scaleup: bool = False,\n",
    "    stride: int = 32,\n",
    "):\n",
    "    \"\"\"\n",
    "    Resize image and padding for detection. Takes image as input,\n",
    "    resizes image to fit into new shape with saving original aspect ratio and pads it to meet stride-multiple constraints\n",
    "\n",
    "    Parameters:\n",
    "      img (np.ndarray): image for preprocessing\n",
    "      new_shape (Tuple(int, int)): image size after preprocessing in format [height, width]\n",
    "      color (Tuple(int, int, int)): color for filling padded area\n",
    "      auto (bool): use dynamic input size, only padding for stride constrins applied\n",
    "      scale_fill (bool): scale image to fill new_shape\n",
    "      scaleup (bool): allow scale image if it is lower then desired input size, can affect model accuracy\n",
    "      stride (int): input padding stride\n",
    "    Returns:\n",
    "      img (np.ndarray): image after preprocessing\n",
    "      ratio (Tuple(float, float)): hight and width scaling ratio\n",
    "      padding_size (Tuple(int, int)): height and width padding size\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    # Resize and pad image while meeting stride-multiple constraints\n",
    "    shape = img.shape[:2]  # current shape [height, width]\n",
    "    if isinstance(new_shape, int):\n",
    "        new_shape = (new_shape, new_shape)\n",
    "\n",
    "    # Scale ratio (new / old)\n",
    "    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
    "    if not scaleup:  # only scale down, do not scale up (for better test mAP)\n",
    "        r = min(r, 1.0)\n",
    "\n",
    "    # Compute padding\n",
    "    ratio = r, r  # width, height ratios\n",
    "    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
    "    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n",
    "    if auto:  # minimum rectangle\n",
    "        dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding\n",
    "    elif scale_fill:  # stretch\n",
    "        dw, dh = 0.0, 0.0\n",
    "        new_unpad = (new_shape[1], new_shape[0])\n",
    "        ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios\n",
    "\n",
    "    dw /= 2  # divide padding into 2 sides\n",
    "    dh /= 2\n",
    "\n",
    "    if shape[::-1] != new_unpad:  # resize\n",
    "        img = cv2.resize(img, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
    "    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
    "    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
    "    img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border\n",
    "    return img, ratio, (dw, dh)\n",
    "\n",
    "\n",
    "def postprocess(\n",
    "    pred_boxes: np.ndarray,\n",
    "    input_hw: Tuple[int, int],\n",
    "    orig_img: np.ndarray,\n",
    "    min_conf_threshold: float = 0.25,\n",
    "    nms_iou_threshold: float = 0.7,\n",
    "    agnosting_nms: bool = False,\n",
    "    max_detections: int = 300,\n",
    "):\n",
    "    \"\"\"\n",
    "    YOLOv8 model postprocessing function. Applied non maximum supression algorithm to detections and rescale boxes to original image size\n",
    "    Parameters:\n",
    "        pred_boxes (np.ndarray): model output prediction boxes\n",
    "        input_hw (np.ndarray): preprocessed image\n",
    "        orig_image (np.ndarray): image before preprocessing\n",
    "        min_conf_threshold (float, *optional*, 0.25): minimal accepted confidence for object filtering\n",
    "        nms_iou_threshold (float, *optional*, 0.45): minimal overlap score for removing objects duplicates in NMS\n",
    "        agnostic_nms (bool, *optiona*, False): apply class agnostinc NMS approach or not\n",
    "        max_detections (int, *optional*, 300):  maximum detections after NMS\n",
    "    Returns:\n",
    "       pred (List[Dict[str, np.ndarray]]): list of dictionary with det - detected boxes in format [x1, y1, x2, y2, score, label]\n",
    "    \"\"\"\n",
    "    nms_kwargs = {\"agnostic\": agnosting_nms, \"max_det\": max_detections}\n",
    "    preds = ops.non_max_suppression(torch.from_numpy(pred_boxes), min_conf_threshold, nms_iou_threshold, nc=80, **nms_kwargs)\n",
    "\n",
    "    results = []\n",
    "    for i, pred in enumerate(preds):\n",
    "        shape = orig_img[i].shape if isinstance(orig_img, list) else orig_img.shape\n",
    "        if not len(pred):\n",
    "            results.append({\"det\": [], \"segment\": []})\n",
    "            continue\n",
    "        pred[:, :4] = ops.scale_boxes(input_hw, pred[:, :4], shape).round()\n",
    "        results.append({\"det\": pred})\n",
    "\n",
    "    return results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1b3f82cc-b20f-46d7-88fe-d3051cfbfb4e",
   "metadata": {},
   "source": [
    "Now, we can skip these preprocessing steps in detect function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8177583e-510f-4e43-be26-253e5d27621e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_without_preprocess(image: np.ndarray, model: ov.Model):\n",
    "    \"\"\"\n",
    "    OpenVINO YOLOv8 model with integrated preprocessing inference function. Preprocess image, runs model inference and postprocess results using NMS.\n",
    "    Parameters:\n",
    "        image (np.ndarray): input image.\n",
    "        model (Model): OpenVINO compiled model.\n",
    "    Returns:\n",
    "        detections (np.ndarray): detected boxes in format [x1, y1, x2, y2, score, label]\n",
    "    \"\"\"\n",
    "    output_layer = model.output(0)\n",
    "    img = letterbox(image)[0]\n",
    "    input_tensor = np.expand_dims(img, 0)\n",
    "    input_hw = img.shape[:2]\n",
    "    result = model(input_tensor)[output_layer]\n",
    "    detections = postprocess(result, input_hw, image)\n",
    "    return detections\n",
    "\n",
    "\n",
    "compiled_model = core.compile_model(quantized_model_with_preprocess, device.value)\n",
    "input_image = np.array(Image.open(IMAGE_PATH))\n",
    "detections = detect_without_preprocess(input_image, compiled_model)[0]\n",
    "image_with_boxes = draw_results(detections, input_image, label_map)\n",
    "\n",
    "Image.fromarray(image_with_boxes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2e3b4862-c182-4ce4-a473-9f38d98deab8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Live demo\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "The following code runs model inference on a video:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7bb92e-e301-45a9-b5ff-f7953fad298f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import time\n",
    "from IPython import display\n",
    "\n",
    "\n",
    "# Main processing function to run object detection.\n",
    "def run_object_detection(\n",
    "    source=0,\n",
    "    flip=False,\n",
    "    use_popup=False,\n",
    "    skip_first_frames=0,\n",
    "    model=det_model,\n",
    "    device=device.value,\n",
    "):\n",
    "    player = None\n",
    "    ov_config = {}\n",
    "    if device != \"CPU\":\n",
    "        model.reshape({0: [1, 3, 640, 640]})\n",
    "    if \"GPU\" in device or (\"AUTO\" in device and \"GPU\" in core.available_devices):\n",
    "        ov_config = {\"GPU_DISABLE_WINOGRAD_CONVOLUTION\": \"YES\"}\n",
    "    compiled_model = core.compile_model(model, device, ov_config)\n",
    "\n",
    "    def infer(*args):\n",
    "        result = compiled_model(args)\n",
    "        return torch.from_numpy(result[0])\n",
    "\n",
    "    det_model.predictor.inference = infer\n",
    "\n",
    "    try:\n",
    "        # Create a video player to play with target fps.\n",
    "        player = VideoPlayer(source=source, flip=flip, fps=30, skip_first_frames=skip_first_frames)\n",
    "        # Start capturing.\n",
    "        player.start()\n",
    "        if use_popup:\n",
    "            title = \"Press ESC to Exit\"\n",
    "            cv2.namedWindow(winname=title, flags=cv2.WINDOW_GUI_NORMAL | cv2.WINDOW_AUTOSIZE)\n",
    "\n",
    "        processing_times = collections.deque()\n",
    "        while True:\n",
    "            # Grab the frame.\n",
    "            frame = player.next()\n",
    "            if frame is None:\n",
    "                print(\"Source ended\")\n",
    "                break\n",
    "            # If the frame is larger than full HD, reduce size to improve the performance.\n",
    "            scale = 1280 / max(frame.shape)\n",
    "            if scale < 1:\n",
    "                frame = cv2.resize(\n",
    "                    src=frame,\n",
    "                    dsize=None,\n",
    "                    fx=scale,\n",
    "                    fy=scale,\n",
    "                    interpolation=cv2.INTER_AREA,\n",
    "                )\n",
    "            # Get the results.\n",
    "            input_image = np.array(frame)\n",
    "\n",
    "            start_time = time.time()\n",
    "            detections = det_model(input_image)\n",
    "            stop_time = time.time()\n",
    "            frame = detections[0].plot()\n",
    "\n",
    "            processing_times.append(stop_time - start_time)\n",
    "            # Use processing times from last 200 frames.\n",
    "            if len(processing_times) > 200:\n",
    "                processing_times.popleft()\n",
    "\n",
    "            _, f_width = frame.shape[:2]\n",
    "            # Mean processing time [ms].\n",
    "            processing_time = np.mean(processing_times) * 1000\n",
    "            fps = 1000 / processing_time\n",
    "            cv2.putText(\n",
    "                img=frame,\n",
    "                text=f\"Inference time: {processing_time:.1f}ms ({fps:.1f} FPS)\",\n",
    "                org=(20, 40),\n",
    "                fontFace=cv2.FONT_HERSHEY_COMPLEX,\n",
    "                fontScale=f_width / 1000,\n",
    "                color=(0, 0, 255),\n",
    "                thickness=1,\n",
    "                lineType=cv2.LINE_AA,\n",
    "            )\n",
    "            # Use this workaround if there is flickering.\n",
    "            if use_popup:\n",
    "                cv2.imshow(winname=title, mat=frame)\n",
    "                key = cv2.waitKey(1)\n",
    "                # escape = 27\n",
    "                if key == 27:\n",
    "                    break\n",
    "            else:\n",
    "                # Encode numpy array to jpg.\n",
    "                _, encoded_img = cv2.imencode(ext=\".jpg\", img=frame, params=[cv2.IMWRITE_JPEG_QUALITY, 100])\n",
    "                # Create an IPython image.\n",
    "                i = display.Image(data=encoded_img)\n",
    "                # Display the image in this notebook.\n",
    "                display.clear_output(wait=True)\n",
    "                display.display(i)\n",
    "    # ctrl-c\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Interrupted\")\n",
    "    # any different error\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "    finally:\n",
    "        if player is not None:\n",
    "            # Stop capturing.\n",
    "            player.stop()\n",
    "        if use_popup:\n",
    "            cv2.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cc5b4ba6-f478-4417-b09d-93fee5adca41",
   "metadata": {},
   "source": [
    "### Run Live Object Detection\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Use a webcam as the video input. By default, the primary webcam is set with `source=0`. If you have multiple webcams, each one will be assigned a consecutive number starting at 0. Set `flip=True` when using a front-facing camera. Some web browsers, especially Mozilla Firefox, may cause flickering. If you experience flickering, set `use_popup=True`.\n",
    "\n",
    ">**NOTE**: To use this notebook with a webcam, you need to run the notebook on a computer with a webcam. If you run the notebook on a remote server (for example, in Binder or Google Colab service), the webcam will not work. By default, the lower cell will run model inference on a video file. If you want to try live inference on your webcam set `WEBCAM_INFERENCE = True`\n",
    "\n",
    "Run the object detection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90708017",
   "metadata": {},
   "outputs": [],
   "source": [
    "WEBCAM_INFERENCE = False\n",
    "\n",
    "if WEBCAM_INFERENCE:\n",
    "    VIDEO_SOURCE = 0  # Webcam\n",
    "else:\n",
    "    VIDEO_SOURCE = \"https://storage.openvinotoolkit.org/repositories/openvino_notebooks/data/data/video/people.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee30d986",
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de91e4d-321f-46fe-a1ad-425e2a04b880",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_object_detection(\n",
    "    source=VIDEO_SOURCE,\n",
    "    flip=True,\n",
    "    use_popup=False,\n",
    "    model=det_ov_model,\n",
    "    device=device.value,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "openvino_notebooks": {
   "imageUrl": "https://user-images.githubusercontent.com/29454499/212105105-f61c8aab-c1ff-40af-a33f-d0ed1fccc72e.png",
   "tags": {
    "categories": [
     "Model Demos",
     "AI Trends"
    ],
    "libraries": [],
    "other": [
     "YOLO"
    ],
    "tasks": [
     "Object Detection"
    ]
   }
  },
  "vscode": {
   "interpreter": {
    "hash": "cec18e25feb9469b5ff1085a8097bdcd86db6a4ac301d6aeff87d0f3e7ce4ca5"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
