{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a2e7d62b-5779-4211-822c-457c77321f8b",
   "metadata": {},
   "source": [
    "# Convert and Optimize YOLOv8 keypoint detection model with OpenVINO™\n",
    "\n",
    "Keypoint detection/Pose is a task that involves detecting specific points in an image or video frame. These points are referred to as keypoints and are used to track movement or pose estimation. YOLOv8 can detect keypoints in an image or video frame with high accuracy and speed.\n",
    "\n",
    "\n",
    "This tutorial demonstrates step-by-step instructions on how to run and optimize [PyTorch YOLOv8 Pose model](https://docs.ultralytics.com/tasks/pose/) with OpenVINO. We consider the steps required for keypoint detection scenario.\n",
    "\n",
    "The tutorial consists of the following steps:\n",
    "- Prepare the PyTorch model.\n",
    "- Download and prepare a dataset.\n",
    "- Validate the original model.\n",
    "- Convert the PyTorch model to OpenVINO IR.\n",
    "- Validate the converted model.\n",
    "- Prepare and run optimization pipeline.\n",
    "- Compare performance of the FP32 and quantized models.\n",
    "- Compare accuracy of the FP32 and quantized models.\n",
    "- Live demo\n",
    "\n",
    "\n",
    "#### Table of contents:\n",
    "\n",
    "- [Get PyTorch model](#Get-PyTorch-model)\n",
    "    - [Prerequisites](#Prerequisites)\n",
    "- [Instantiate model](#Instantiate-model)\n",
    "    - [Convert model to OpenVINO IR](#Convert-model-to-OpenVINO-IR)\n",
    "    - [Verify model inference](#Verify-model-inference)\n",
    "    - [Select inference device](#Select-inference-device)\n",
    "    - [Test on single image](#Test-on-single-image)\n",
    "- [Check model accuracy on the dataset](#Check-model-accuracy-on-the-dataset)\n",
    "    - [Download the validation dataset](#Download-the-validation-dataset)\n",
    "    - [Define validation function](#Define-validation-function)\n",
    "    - [Configure Validator helper and create DataLoader](#Configure-Validator-helper-and-create-DataLoader)\n",
    "- [Optimize model using NNCF Post-training Quantization API](#Optimize-model-using-NNCF-Post-training-Quantization-API)\n",
    "    - [Validate Quantized model inference](#Validate-Quantized-model-inference)\n",
    "- [Compare the Original and Quantized Models](#Compare-the-Original-and-Quantized-Models)\n",
    "    - [Compare performance of the Original and Quantized Models](#Compare-performance-of-the-Original-and-Quantized-Models)\n",
    "    - [Compare accuracy of the Original and Quantized Models](#Compare-accuracy-of-the-Original-and-Quantized-Models)\n",
    "- [Other ways to optimize model](#Other-ways-to-optimize-model)\n",
    "- [Live demo](#Live-demo)\n",
    "    - [Run Keypoint Detection on video](#Run-Keypoint-Detection-on-video)\n",
    "\n",
    "\n",
    "### Installation Instructions\n",
    "\n",
    "This is a self-contained example that relies solely on its own code.\n",
    "\n",
    "We recommend  running the notebook in a virtual environment. You only need a Jupyter server to start.\n",
    "For details, please refer to [Installation Guide](https://github.com/openvinotoolkit/openvino_notebooks/blob/latest/README.md#-installation-guide).\n",
    "\n",
    "<img referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=5b5a4db0-7875-4bfb-bdbd-01698b5b1a77&file=notebooks/yolov8-optimization/yolov8-keypoint-detection.ipynb\" />\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d7a12678-b12f-48d1-9735-398855733e46",
   "metadata": {},
   "source": [
    "## Get PyTorch model\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Generally, PyTorch models represent an instance of the [`torch.nn.Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) class, initialized by a state dictionary with model weights.\n",
    "We will use the YOLOv8 nano model (also known as `yolov8n`) pre-trained on a COCO dataset, which is available in this [repo](https://github.com/ultralytics/ultralytics). Similar steps are also applicable to other YOLOv8 models.\n",
    "Typical steps to obtain a pre-trained model:\n",
    "1. Create an instance of a model class.\n",
    "2. Load a checkpoint state dict, which contains the pre-trained model weights.\n",
    "3. Turn the model to evaluation for switching some operations to inference mode.\n",
    "\n",
    "In this case, the creators of the model provide an API that enables converting the YOLOv8 model to OpenVINO IR. Therefore, we do not need to do these steps manually."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e2267760-cbfe-41c6-958d-cad9f845d5bb",
   "metadata": {},
   "source": [
    "#### Prerequisites\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Install necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d04872-6916-454c-9211-6c644b50dc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q \"openvino>=2024.0.0\" \"nncf>=2.9.0\"\n",
    "%pip install -q \"protobuf==3.20.*\" \"torch>=2.1\" \"torchvision>=0.16\" \"ultralytics==8.2.24\" \"onnx\" tqdm opencv-python --extra-index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1bbe319c",
   "metadata": {},
   "source": [
    "Import required utility functions.\n",
    "The lower cell will download the `notebook_utils` Python module from GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f6cd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Fetch `notebook_utils` module\n",
    "import requests\n",
    "\n",
    "r = requests.get(\n",
    "    url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/notebook_utils.py\",\n",
    ")\n",
    "\n",
    "open(\"notebook_utils.py\", \"w\").write(r.text)\n",
    "from notebook_utils import download_file, VideoPlayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373658bd-7e64-4479-914e-f2742d330afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download a test sample\n",
    "IMAGE_PATH = Path(\"./data/intel_rnb.jpg\")\n",
    "download_file(\n",
    "    url=\"https://storage.openvinotoolkit.org/repositories/openvino_notebooks/data/data/image/intel_rnb.jpg\",\n",
    "    filename=IMAGE_PATH.name,\n",
    "    directory=IMAGE_PATH.parent,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ee32fd08-650c-4751-bb41-d8afccb2495e",
   "metadata": {},
   "source": [
    "## Instantiate model\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "For loading the model, required to specify a path to the model checkpoint. It can be some local path or name available on models hub (in this case model checkpoint will be downloaded automatically).\n",
    "\n",
    "Making prediction, the model accepts a path to input image and returns list with Results class object. Results contains boxes and key points. Also it contains utilities for processing results, for example, `plot()` method for drawing.\n",
    "\n",
    "Let us consider the examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae2543b",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = Path(\"./models\")\n",
    "models_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fdb05e-02a6-48f6-ac64-7199f0c331fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from ultralytics import YOLO\n",
    "\n",
    "POSE_MODEL_NAME = \"yolov8n-pose\"\n",
    "\n",
    "pose_model = YOLO(models_dir / f\"{POSE_MODEL_NAME}.pt\")\n",
    "label_map = pose_model.model.names\n",
    "\n",
    "res = pose_model(IMAGE_PATH)\n",
    "Image.fromarray(res[0].plot()[:, :, ::-1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e345ffcc-c4b8-44ba-8b03-f37e63a060da",
   "metadata": {},
   "source": [
    "### Convert model to OpenVINO IR\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "YOLOv8 provides API for convenient model exporting to different formats including OpenVINO IR. `model.export` is responsible for model conversion. We need to specify the format, and additionally, we can preserve dynamic shapes in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2cc576-50c9-409f-be86-ad7122dce24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# object detection model\n",
    "pose_model_path = models_dir / f\"{POSE_MODEL_NAME}_openvino_model/{POSE_MODEL_NAME}.xml\"\n",
    "if not pose_model_path.exists():\n",
    "    pose_model.export(format=\"openvino\", dynamic=True, half=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "713cd45f-2b19-4a1e-bc9d-5e69bd95d896",
   "metadata": {},
   "source": [
    "### Verify model inference\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "We can reuse the base model pipeline for pre- and postprocessing just replacing the inference method where we will use the IR model for inference."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "83167bea",
   "metadata": {},
   "source": [
    "### Select inference device\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Select device from dropdown list for running inference using OpenVINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2841f0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "import openvino as ov\n",
    "\n",
    "core = ov.Core()\n",
    "\n",
    "device = widgets.Dropdown(\n",
    "    options=core.available_devices + [\"AUTO\"],\n",
    "    value=\"AUTO\",\n",
    "    description=\"Device:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cd163eab-e803-4ae8-96da-22c3bf303630",
   "metadata": {},
   "source": [
    "### Test on single image\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Now, once we have defined preprocessing and postprocessing steps, we are ready to check model prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97b41c0-02d4-4357-90a1-9b727f0fcf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "core = ov.Core()\n",
    "pose_ov_model = core.read_model(pose_model_path)\n",
    "\n",
    "ov_config = {}\n",
    "if device.value != \"CPU\":\n",
    "    pose_ov_model.reshape({0: [1, 3, 640, 640]})\n",
    "if \"GPU\" in device.value or (\"AUTO\" in device.value and \"GPU\" in core.available_devices):\n",
    "    ov_config = {\"GPU_DISABLE_WINOGRAD_CONVOLUTION\": \"YES\"}\n",
    "pose_compiled_model = core.compile_model(pose_ov_model, device.value, ov_config)\n",
    "\n",
    "\n",
    "def infer(*args):\n",
    "    result = pose_compiled_model(args)\n",
    "    return torch.from_numpy(result[0])\n",
    "\n",
    "\n",
    "pose_model.predictor.inference = infer\n",
    "pose_model.predictor.model.pt = False\n",
    "\n",
    "res = pose_model(IMAGE_PATH)\n",
    "Image.fromarray(res[0].plot()[:, :, ::-1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7e918cf6-cdce-4e90-a6d7-c435a3c08d93",
   "metadata": {},
   "source": [
    "Great! The result is the same, as produced by original models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ae5b0855-4bcd-46cc-a0a8-f13a693922e9",
   "metadata": {},
   "source": [
    "## Check model accuracy on the dataset\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "For comparing the optimized model result with the original, it is good to know some measurable results in terms of model accuracy on the validation dataset. \n",
    "\n",
    "\n",
    "### Download the validation dataset\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "YOLOv8 is pre-trained on the COCO dataset, so to evaluate the model accuracy we need to download it. According to the instructions provided in the YOLOv8 repo, we also need to download annotations in the format used by the author of the model, for use with the original model evaluation function.\n",
    "\n",
    ">**Note**: The initial dataset download may take a few minutes to complete. The download speed will vary depending on the quality of your internet connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a3a3a2-5141-4cfd-bbf1-84dbb5e0bc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "\n",
    "from ultralytics.data.utils import DATASETS_DIR\n",
    "\n",
    "\n",
    "DATA_URL = \"https://ultralytics.com/assets/coco8-pose.zip\"\n",
    "CFG_URL = \"https://raw.githubusercontent.com/ultralytics/ultralytics/v8.1.0/ultralytics/cfg/datasets/coco8-pose.yaml\"\n",
    "\n",
    "OUT_DIR = DATASETS_DIR\n",
    "\n",
    "DATA_PATH = OUT_DIR / \"val2017.zip\"\n",
    "CFG_PATH = OUT_DIR / \"coco8-pose.yaml\"\n",
    "\n",
    "download_file(DATA_URL, DATA_PATH.name, DATA_PATH.parent)\n",
    "download_file(CFG_URL, CFG_PATH.name, CFG_PATH.parent)\n",
    "\n",
    "if not (OUT_DIR / \"coco8-pose/labels\").exists():\n",
    "    with ZipFile(DATA_PATH, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(OUT_DIR)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0f516ba2-e54c-416c-893a-cbcd2228e140",
   "metadata": {},
   "source": [
    "### Define validation function\n",
    "[back to top ⬆️](#Table-of-contents:)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96bd01e-5b92-49be-b7b8-9269ace1398b",
   "metadata": {
    "test_replace": {
     "int = None": "int = 100"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from ultralytics.utils.metrics import ConfusionMatrix\n",
    "\n",
    "\n",
    "def test(\n",
    "    model: ov.Model,\n",
    "    core: ov.Core,\n",
    "    data_loader: torch.utils.data.DataLoader,\n",
    "    validator,\n",
    "    num_samples: int = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    OpenVINO YOLOv8 model accuracy validation function. Runs model validation on dataset and returns metrics\n",
    "    Parameters:\n",
    "        model (Model): OpenVINO model\n",
    "        data_loader (torch.utils.data.DataLoader): dataset loader\n",
    "        validator: instance of validator class\n",
    "        num_samples (int, *optional*, None): validate model only on specified number samples, if provided\n",
    "    Returns:\n",
    "        stats: (Dict[str, float]) - dictionary with aggregated accuracy metrics statistics, key is metric name, value is metric value\n",
    "    \"\"\"\n",
    "    validator.seen = 0\n",
    "    validator.jdict = []\n",
    "    validator.stats = dict(tp_p=[], tp=[], conf=[], pred_cls=[], target_cls=[])\n",
    "    validator.batch_i = 1\n",
    "    validator.confusion_matrix = ConfusionMatrix(nc=validator.nc)\n",
    "    model.reshape({0: [1, 3, -1, -1]})\n",
    "    compiled_model = core.compile_model(model)\n",
    "    for batch_i, batch in enumerate(tqdm(data_loader, total=num_samples)):\n",
    "        if num_samples is not None and batch_i == num_samples:\n",
    "            break\n",
    "        batch = validator.preprocess(batch)\n",
    "        results = compiled_model(batch[\"img\"])\n",
    "        preds = torch.from_numpy(results[compiled_model.output(0)])\n",
    "        preds = validator.postprocess(preds)\n",
    "        validator.update_metrics(preds, batch)\n",
    "    stats = validator.get_stats()\n",
    "    return stats\n",
    "\n",
    "\n",
    "def print_stats(stats: np.ndarray, total_images: int, total_objects: int):\n",
    "    \"\"\"\n",
    "    Helper function for printing accuracy statistic\n",
    "    Parameters:\n",
    "        stats: (Dict[str, float]) - dictionary with aggregated accuracy metrics statistics, key is metric name, value is metric value\n",
    "        total_images (int) -  number of evaluated images\n",
    "        total objects (int)\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    print(\"Boxes:\")\n",
    "    mp, mr, map50, mean_ap = (\n",
    "        stats[\"metrics/precision(B)\"],\n",
    "        stats[\"metrics/recall(B)\"],\n",
    "        stats[\"metrics/mAP50(B)\"],\n",
    "        stats[\"metrics/mAP50-95(B)\"],\n",
    "    )\n",
    "    # Print results\n",
    "    print(\"    Best mean average:\")\n",
    "    s = (\"%20s\" + \"%12s\" * 6) % (\n",
    "        \"Class\",\n",
    "        \"Images\",\n",
    "        \"Labels\",\n",
    "        \"Precision\",\n",
    "        \"Recall\",\n",
    "        \"mAP@.5\",\n",
    "        \"mAP@.5:.95\",\n",
    "    )\n",
    "    print(s)\n",
    "    pf = \"%20s\" + \"%12i\" * 2 + \"%12.3g\" * 4  # print format\n",
    "    print(pf % (\"all\", total_images, total_objects, mp, mr, map50, mean_ap))\n",
    "    if \"metrics/precision(M)\" in stats:\n",
    "        s_mp, s_mr, s_map50, s_mean_ap = (\n",
    "            stats[\"metrics/precision(M)\"],\n",
    "            stats[\"metrics/recall(M)\"],\n",
    "            stats[\"metrics/mAP50(M)\"],\n",
    "            stats[\"metrics/mAP50-95(M)\"],\n",
    "        )\n",
    "        # Print results\n",
    "        print(\"    Macro average mean:\")\n",
    "        s = (\"%20s\" + \"%12s\" * 6) % (\n",
    "            \"Class\",\n",
    "            \"Images\",\n",
    "            \"Labels\",\n",
    "            \"Precision\",\n",
    "            \"Recall\",\n",
    "            \"mAP@.5\",\n",
    "            \"mAP@.5:.95\",\n",
    "        )\n",
    "        print(s)\n",
    "        pf = \"%20s\" + \"%12i\" * 2 + \"%12.3g\" * 4  # print format\n",
    "        print(pf % (\"all\", total_images, total_objects, s_mp, s_mr, s_map50, s_mean_ap))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "41ee1a66-c2ca-413b-883c-497f718337e6",
   "metadata": {},
   "source": [
    "### Configure Validator helper and create DataLoader\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "The original model repository uses a `Validator` wrapper, which represents the accuracy validation pipeline. It creates dataloader and evaluation metrics and updates metrics on each data batch produced by the dataloader. Besides that, it is responsible for data preprocessing and results postprocessing. For class initialization, the configuration should be provided. We will use the default setup, but it can be replaced with some parameters overriding to test on custom data. The model has connected the `ValidatorClass` method, which creates a validator class instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878473e3-7638-4471-bd8e-31350b119f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics.utils import DEFAULT_CFG\n",
    "from ultralytics.cfg import get_cfg\n",
    "from ultralytics.data.utils import check_det_dataset\n",
    "\n",
    "args = get_cfg(cfg=DEFAULT_CFG)\n",
    "args.data = \"coco8-pose.yaml\"\n",
    "args.model = \"yolov8n-pose.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466729b5-4fad-4de2-a1d4-2952718cbe7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics.models.yolo.pose import PoseValidator\n",
    "\n",
    "pose_validator = PoseValidator(args=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccd7ba9-6066-4328-9563-4f1a740bca9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pose_validator.data = check_det_dataset(args.data)\n",
    "pose_validator.stride = 32\n",
    "pose_data_loader = pose_validator.get_dataloader(OUT_DIR / \"coco8-pose\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1170c0b3-86f5-4908-ac80-6df130024b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics.utils.metrics import OKS_SIGMA\n",
    "\n",
    "pose_validator.is_coco = True\n",
    "pose_validator.names = pose_model.model.names\n",
    "pose_validator.metrics.names = pose_validator.names\n",
    "pose_validator.nc = pose_model.model.model[-1].nc\n",
    "pose_validator.sigma = OKS_SIGMA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "87a73bbc-d0e0-494e-9533-26d51295a196",
   "metadata": {},
   "source": [
    "After definition test function and validator creation, we are ready for getting accuracy metrics\n",
    ">**Note**: Model evaluation is time consuming process and can take several minutes, depending on the hardware. For reducing calculation time, we define `num_samples` parameter with evaluation subset size, but in this case, accuracy can be noncomparable with originally reported by the authors of the model, due to validation subset difference.\n",
    "*To validate the models on the full dataset set `NUM_TEST_SAMPLES = None`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e7bb1e",
   "metadata": {
    "test_replace": {
     "NUM_TEST_SAMPLES = 300": "NUM_TEST_SAMPLES = 4"
    }
   },
   "outputs": [],
   "source": [
    "NUM_TEST_SAMPLES = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d2df4b-4e03-4c5d-a329-9fae35fdf7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_pose_stats = test(pose_ov_model, core, pose_data_loader, pose_validator, num_samples=NUM_TEST_SAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b396bb40-9c0c-48c3-a485-c4642dc6bf1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_stats(fp_pose_stats, pose_validator.seen, pose_validator.nt_per_class.sum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8d7f89f3-4a8f-4b1c-a582-964e111e51dc",
   "metadata": {},
   "source": [
    "`print_stats` reports the following list of accuracy metrics:\n",
    "\n",
    "* `Precision` is the degree of exactness of the model in identifying only relevant objects. \n",
    "* `Recall` measures the ability of the model to detect all ground truths objects.\n",
    "* `mAP@t` - mean average precision, represented as area under the Precision-Recall curve aggregated over all classes in the dataset, where `t` is the Intersection Over Union (IOU) threshold, degree of overlapping between ground truth and predicted objects. Therefore, `mAP@.5` indicates that mean average precision is calculated at 0.5 IOU threshold, `mAP@.5:.95` - is calculated on range IOU thresholds from 0.5 to 0.95 with step 0.05."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "13b69e12-2e22-44c1-bfed-fdc88f6c424d",
   "metadata": {},
   "source": [
    "## Optimize model using NNCF Post-training Quantization API\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "[NNCF](https://github.com/openvinotoolkit/nncf) provides a suite of advanced algorithms for Neural Networks inference optimization in OpenVINO with minimal accuracy drop.\n",
    "We will use 8-bit quantization in post-training mode (without the fine-tuning pipeline) to optimize YOLOv8.\n",
    "\n",
    "The optimization process contains the following steps:\n",
    "\n",
    "1. Create a Dataset for quantization.\n",
    "2. Run `nncf.quantize` for getting an optimized model.\n",
    "3. Serialize OpenVINO IR model, using the `openvino.runtime.serialize` function."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e7aa78cd-ef51-4c59-9a70-09ff82307c80",
   "metadata": {},
   "source": [
    "Please select below whether you would like to run quantization to improve model inference speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3f2a40-a7e4-4be7-a037-07440fed48a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "int8_model_pose_path = models_dir / f\"{POSE_MODEL_NAME}_openvino_int8_model/{POSE_MODEL_NAME}.xml\"\n",
    "\n",
    "to_quantize = widgets.Checkbox(\n",
    "    value=True,\n",
    "    description=\"Quantization\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "to_quantize"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c52d8962-79e4-4ac1-962b-f43e8088e2aa",
   "metadata": {},
   "source": [
    "Let's load `skip magic` extension to skip quantization if `to_quantize` is not selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3c3563-24bc-440f-9715-0e8532492e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch skip_kernel_extension module\n",
    "r = requests.get(\n",
    "    url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/skip_kernel_extension.py\",\n",
    ")\n",
    "open(\"skip_kernel_extension.py\", \"w\").write(r.text)\n",
    "\n",
    "%load_ext skip_kernel_extension"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7d7eeae8-f3f4-4e95-b1a6-a23085f03ebc",
   "metadata": {},
   "source": [
    "Reuse validation dataloader in accuracy testing for quantization. \n",
    "For that, it should be wrapped into the `nncf.Dataset` object and define a transformation function for getting only input tensors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd994958-6988-4a1d-ac7f-3efbd97135cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip not $to_quantize.value\n",
    "\n",
    "import nncf\n",
    "from typing import Dict\n",
    "\n",
    "\n",
    "def transform_fn(data_item:Dict):\n",
    "    \"\"\"\n",
    "    Quantization transform function. Extracts and preprocess input data from dataloader item for quantization.\n",
    "    Parameters:\n",
    "       data_item: Dict with data item produced by DataLoader during iteration\n",
    "    Returns:\n",
    "        input_tensor: Input data for quantization\n",
    "    \"\"\"\n",
    "    input_tensor = pose_validator.preprocess(data_item)['img'].numpy()\n",
    "    return input_tensor\n",
    "\n",
    "\n",
    "quantization_dataset = nncf.Dataset(pose_data_loader, transform_fn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "91629284-e261-494d-9464-146ed7190084",
   "metadata": {},
   "source": [
    "The `nncf.quantize` function provides an interface for model quantization. It requires an instance of the OpenVINO Model and quantization dataset. \n",
    "Optionally, some additional parameters for the configuration quantization process (number of samples for quantization, preset, ignored scope, etc.) can be provided. YOLOv8 model contains non-ReLU activation functions, which require asymmetric quantization of activations. To achieve a better result, we will use a `mixed` quantization preset. It provides symmetric quantization of weights and asymmetric quantization of activations. For more accurate results, we should keep the operation in the postprocessing subgraph in floating point precision, using the `ignored_scope` parameter.\n",
    "\n",
    ">**Note**: Model post-training quantization is time-consuming process. Be patient, it can take several minutes depending on your hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e8cec4-d0b3-4da0-b54c-7964e7bcbfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip not $to_quantize.value\n",
    "\n",
    "ignored_scope = nncf.IgnoredScope(\n",
    "    subgraphs=[\n",
    "        nncf.Subgraph(inputs=['__module.model.22/aten::cat/Concat',\n",
    "                              '__module.model.22/aten::cat/Concat_1',\n",
    "                              '__module.model.22/aten::cat/Concat_2',\n",
    "                             '__module.model.22/aten::cat/Concat_7'],\n",
    "                      outputs=['__module.model.22/aten::cat/Concat_9'])\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Detection model\n",
    "quantized_pose_model = nncf.quantize(\n",
    "    pose_ov_model,\n",
    "    quantization_dataset,\n",
    "    preset=nncf.QuantizationPreset.MIXED,\n",
    "    ignored_scope=ignored_scope\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab02faf1-196c-4f91-93d1-c86b4cae93d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip not $to_quantize.value\n",
    "\n",
    "print(f\"Quantized keypoint detection model will be saved to {int8_model_pose_path}\")\n",
    "ov.save_model(quantized_pose_model, str(int8_model_pose_path))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c8c92a68-3eb8-4ea5-9e35-d496f45b3cc3",
   "metadata": {},
   "source": [
    "### Validate Quantized model inference\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "`nncf.quantize` returns the OpenVINO Model class instance, which is suitable for loading on a device for making predictions. `INT8` model input data and output result formats have no difference from the floating point model representation. Therefore, we can reuse the same `detect` function defined above for getting the `INT8` model result on the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f52267",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip not $to_quantize.value\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2273f037-d091-4d63-af1e-eed6b41fa6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip not $to_quantize.value\n",
    "\n",
    "ov_config = {}\n",
    "if device.value != \"CPU\":\n",
    "    quantized_pose_model.reshape({0: [1, 3, 640, 640]})\n",
    "if \"GPU\" in device.value or (\"AUTO\" in device.value and \"GPU\" in core.available_devices):\n",
    "    ov_config = {\"GPU_DISABLE_WINOGRAD_CONVOLUTION\": \"YES\"}\n",
    "quantized_pose_compiled_model = core.compile_model(quantized_pose_model, device.value, ov_config)\n",
    "\n",
    "def infer(*args):\n",
    "    result = quantized_pose_compiled_model(args)\n",
    "    return torch.from_numpy(result[0])\n",
    "\n",
    "pose_model.predictor.inference = infer\n",
    "\n",
    "res = pose_model(IMAGE_PATH)\n",
    "display(Image.fromarray(res[0].plot()[:, :, ::-1]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0679e2a9",
   "metadata": {},
   "source": [
    "## Compare the Original and Quantized Models\n",
    "[back to top ⬆️](#Table-of-contents:)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "59081cae-9c92-46f0-8117-ed8c6fa6ff19",
   "metadata": {},
   "source": [
    "### Compare performance of the Original and Quantized Models\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "Finally, use the OpenVINO [Benchmark Tool](https://docs.openvino.ai/2024/learn-openvino/openvino-samples/benchmark-tool.html) to measure the inference performance of the `FP32` and `INT8` models.\n",
    "\n",
    "> **Note**: For more accurate performance, it is recommended to run `benchmark_app` in a terminal/command prompt after closing other applications. Run `benchmark_app -m <model_path> -d CPU -shape \"<input_shape>\"` to benchmark async inference on CPU on specific input data shape for one minute. Change `CPU` to `GPU` to benchmark on GPU. Run `benchmark_app --help` to see an overview of all command-line options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e9035a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip not $to_quantize.value\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b07129-ec21-40e0-9885-1928ace8a55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if int8_model_pose_path.exists():\n",
    "    # Inference FP32 model (OpenVINO IR)\n",
    "    !benchmark_app -m $pose_model_path -d $device.value -api async -shape \"[1,3,640,640]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646cafce-8042-4b94-aead-de804b2051a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if int8_model_pose_path.exists():\n",
    "    # Inference INT8 model (OpenVINO IR)\n",
    "    !benchmark_app -m $int8_model_pose_path -d $device.value -api async -shape \"[1,3,640,640]\" -t 15"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f800b975-d001-4947-ac20-6be9fd737a95",
   "metadata": {},
   "source": [
    "### Compare accuracy of the Original and Quantized Models\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "As we can see, there is no significant difference between `INT8` and float model result in a single image test. To understand how quantization influences model prediction precision, we can compare model accuracy on a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48299ffc-99b0-4b27-b365-6b0b8beabd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip not $to_quantize.value\n",
    "\n",
    "int8_pose_stats = test(quantized_pose_model, core, pose_data_loader, pose_validator, num_samples=NUM_TEST_SAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150e5f85-cf0f-4362-9c9e-7be8957ccc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip not $to_quantize.value\n",
    "\n",
    "print(\"FP32 model accuracy\")\n",
    "print_stats(fp_pose_stats, pose_validator.seen, pose_validator.nt_per_class.sum())\n",
    "\n",
    "print(\"INT8 model accuracy\")\n",
    "print_stats(int8_pose_stats, pose_validator.seen, pose_validator.nt_per_class.sum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d4a8dd76-3de3-4d4c-baef-bfd90400b127",
   "metadata": {},
   "source": [
    "Great! Looks like accuracy was changed, but not significantly and it meets passing criteria."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "184bcebd-588d-4b4d-a60a-0fa753a07ef9",
   "metadata": {},
   "source": [
    "## Other ways to optimize model\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "The performance could be also improved by another OpenVINO method such as async inference pipeline or preprocessing API. \n",
    "\n",
    "Async Inference pipeline help to utilize the device more optimal. The key advantage of the Async API is that when a device is busy with inference, the application can perform other tasks in parallel (for example, populating inputs or scheduling other requests) rather than wait for the current inference to complete first. To understand how to perform async inference using openvino, refer to [Async API tutorial](../async-api/async-api.ipynb)\n",
    "\n",
    "Preprocessing API enables making preprocessing a part of the model reducing application code and dependency on additional image processing libraries. \n",
    "The main advantage of Preprocessing API is that preprocessing steps will be integrated into the execution graph and will be performed on a selected device (CPU/GPU etc.) rather than always being executed on CPU as part of an application. This will also improve selected device utilization. For more information, refer to the overview of [Preprocessing API tutorial](../optimize-preprocessing/optimize-preprocessing.ipynb). To see, how it could be used with YOLOV8 object detection model , please, see [Convert and Optimize YOLOv8 real-time object detection with OpenVINO tutorial](./yolov8-object-detection.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2e3b4862-c182-4ce4-a473-9f38d98deab8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Live demo\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "The following code runs model inference on a video:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7bb92e-e301-45a9-b5ff-f7953fad298f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import time\n",
    "from IPython import display\n",
    "import cv2\n",
    "\n",
    "\n",
    "def run_keypoint_detection(\n",
    "    source=0,\n",
    "    flip=False,\n",
    "    use_popup=False,\n",
    "    skip_first_frames=0,\n",
    "    model=pose_model,\n",
    "    device=device.value,\n",
    "):\n",
    "    player = None\n",
    "\n",
    "    ov_config = {}\n",
    "    if device != \"CPU\":\n",
    "        model.reshape({0: [1, 3, 640, 640]})\n",
    "    if \"GPU\" in device or (\"AUTO\" in device and \"GPU\" in core.available_devices):\n",
    "        ov_config = {\"GPU_DISABLE_WINOGRAD_CONVOLUTION\": \"YES\"}\n",
    "    compiled_model = core.compile_model(model, device, ov_config)\n",
    "\n",
    "    def infer(*args):\n",
    "        result = compiled_model(args)\n",
    "        return torch.from_numpy(result[0])\n",
    "\n",
    "    pose_model.predictor.inference = infer\n",
    "\n",
    "    try:\n",
    "        # Create a video player to play with target fps.\n",
    "        player = VideoPlayer(source=source, flip=flip, fps=30, skip_first_frames=skip_first_frames)\n",
    "        # Start capturing.\n",
    "        player.start()\n",
    "        if use_popup:\n",
    "            title = \"Press ESC to Exit\"\n",
    "            cv2.namedWindow(winname=title, flags=cv2.WINDOW_GUI_NORMAL | cv2.WINDOW_AUTOSIZE)\n",
    "\n",
    "        processing_times = collections.deque()\n",
    "        while True:\n",
    "            # Grab the frame.\n",
    "            frame = player.next()\n",
    "            if frame is None:\n",
    "                print(\"Source ended\")\n",
    "                break\n",
    "            # If the frame is larger than full HD, reduce size to improve the performance.\n",
    "            scale = 1280 / max(frame.shape)\n",
    "            if scale < 1:\n",
    "                frame = cv2.resize(\n",
    "                    src=frame,\n",
    "                    dsize=None,\n",
    "                    fx=scale,\n",
    "                    fy=scale,\n",
    "                    interpolation=cv2.INTER_AREA,\n",
    "                )\n",
    "            # Get the results\n",
    "            input_image = np.array(frame)\n",
    "\n",
    "            start_time = time.time()\n",
    "\n",
    "            detections = pose_model(input_image)\n",
    "            stop_time = time.time()\n",
    "            frame = detections[0].plot()\n",
    "\n",
    "            processing_times.append(stop_time - start_time)\n",
    "            # Use processing times from last 200 frames.\n",
    "            if len(processing_times) > 200:\n",
    "                processing_times.popleft()\n",
    "\n",
    "            _, f_width = frame.shape[:2]\n",
    "            # Mean processing time [ms].\n",
    "            processing_time = np.mean(processing_times) * 1000\n",
    "            fps = 1000 / processing_time\n",
    "            cv2.putText(\n",
    "                img=frame,\n",
    "                text=f\"Inference time: {processing_time:.1f}ms ({fps:.1f} FPS)\",\n",
    "                org=(20, 40),\n",
    "                fontFace=cv2.FONT_HERSHEY_COMPLEX,\n",
    "                fontScale=f_width / 1000,\n",
    "                color=(0, 0, 255),\n",
    "                thickness=1,\n",
    "                lineType=cv2.LINE_AA,\n",
    "            )\n",
    "            # Use this workaround if there is flickering.\n",
    "            if use_popup:\n",
    "                cv2.imshow(winname=title, mat=frame)\n",
    "                key = cv2.waitKey(1)\n",
    "                # escape = 27\n",
    "                if key == 27:\n",
    "                    break\n",
    "            else:\n",
    "                # Encode numpy array to jpg.\n",
    "                _, encoded_img = cv2.imencode(ext=\".jpg\", img=frame, params=[cv2.IMWRITE_JPEG_QUALITY, 100])\n",
    "                # Create an IPython image.\n",
    "                i = display.Image(data=encoded_img)\n",
    "                # Display the image in this notebook.\n",
    "                display.clear_output(wait=True)\n",
    "                display.display(i)\n",
    "    # ctrl-c\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Interrupted\")\n",
    "    # any different error\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "    finally:\n",
    "        if player is not None:\n",
    "            # Stop capturing.\n",
    "            player.stop()\n",
    "        if use_popup:\n",
    "            cv2.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cc5b4ba6-f478-4417-b09d-93fee5adca41",
   "metadata": {},
   "source": [
    "### Run Keypoint Detection on video\n",
    "[back to top ⬆️](#Table-of-contents:)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90708017",
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_SOURCE = \"https://storage.openvinotoolkit.org/repositories/openvino_notebooks/data/data/video/people.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3674eeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de91e4d-321f-46fe-a1ad-425e2a04b880",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_keypoint_detection(\n",
    "    source=VIDEO_SOURCE,\n",
    "    flip=True,\n",
    "    use_popup=False,\n",
    "    model=pose_ov_model,\n",
    "    device=device.value,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "openvino_notebooks": {
   "imageUrl": "https://user-images.githubusercontent.com/29454499/272967083-a36d1072-e2f6-4dd9-85ad-4b2b3b4341d5.png",
   "tags": {
    "categories": [
     "Model Demos",
     "AI Trends"
    ],
    "libraries": [],
    "other": [
     "YOLO"
    ],
    "tasks": [
     "Pose Estimation"
    ]
   }
  },
  "vscode": {
   "interpreter": {
    "hash": "cec18e25feb9469b5ff1085a8097bdcd86db6a4ac301d6aeff87d0f3e7ce4ca5"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
